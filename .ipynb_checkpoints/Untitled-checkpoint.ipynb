{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48e9c839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "import string\n",
    "import pickle\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94732760",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE = 'train.csv'\n",
    "TEST_FILE = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdd80218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_data(TRAINING_FILE):\n",
    "    with open(TRAINING_FILE) as file:\n",
    "        csv_reader = csv.reader(file, delimiter=\",\")\n",
    "        next(csv_reader)\n",
    "        labels = []  # create labels list\n",
    "        images = [] # create images list\n",
    "        for row in csv_reader:\n",
    "            labels.append(float(row[0]))\n",
    "            image = np.array(row[1:]).astype(float)\n",
    "            image = np.reshape(image, (28, 28)) #reshaping the image data\n",
    "            images.append(image)\n",
    "         # return numpy array\n",
    "        return np.array(images).astype(np.float32), np.array(labels).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7ab982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_data(TEST_FILE):\n",
    "    with open(TEST_FILE) as file:\n",
    "        csv_reader = csv.reader(file, delimiter=\",\")\n",
    "        next(csv_reader)\n",
    "        images = []\n",
    "        for row in csv_reader:\n",
    "            image = np.array(row).astype(float)\n",
    "            image = np.reshape(image, (28, 28))\n",
    "            images.append(image)\n",
    "\n",
    "        return np.array(images).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b51b6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopAtAccuracy(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('sparse_categorical_accuracy') > 0.9999:\n",
    "            print(\"\\nReached 99.99% accuracy, stopping training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a77765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bb7dfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.3570 - sparse_categorical_accuracy: 0.8948\n",
      "Epoch 2/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.2143 - sparse_categorical_accuracy: 0.9358\n",
      "Epoch 3/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1856 - sparse_categorical_accuracy: 0.9446\n",
      "Epoch 4/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.1694 - sparse_categorical_accuracy: 0.9494\n",
      "Epoch 5/50\n",
      "1313/1313 [==============================] - 10s 8ms/step - loss: 0.1580 - sparse_categorical_accuracy: 0.9522\n",
      "Epoch 6/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1514 - sparse_categorical_accuracy: 0.9545\n",
      "Epoch 7/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1432 - sparse_categorical_accuracy: 0.9568\n",
      "Epoch 8/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1396 - sparse_categorical_accuracy: 0.9584\n",
      "Epoch 9/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.1361 - sparse_categorical_accuracy: 0.9603\n",
      "Epoch 10/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1280 - sparse_categorical_accuracy: 0.9616\n",
      "Epoch 11/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.1261 - sparse_categorical_accuracy: 0.9615\n",
      "Epoch 12/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1241 - sparse_categorical_accuracy: 0.9629\n",
      "Epoch 13/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.1193 - sparse_categorical_accuracy: 0.9640\n",
      "Epoch 14/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1133 - sparse_categorical_accuracy: 0.9662\n",
      "Epoch 15/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1132 - sparse_categorical_accuracy: 0.9660\n",
      "Epoch 16/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1116 - sparse_categorical_accuracy: 0.9661\n",
      "Epoch 17/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1114 - sparse_categorical_accuracy: 0.9663\n",
      "Epoch 18/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1078 - sparse_categorical_accuracy: 0.9670\n",
      "Epoch 19/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1107 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 20/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1090 - sparse_categorical_accuracy: 0.9682\n",
      "Epoch 21/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1055 - sparse_categorical_accuracy: 0.9683\n",
      "Epoch 22/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1057 - sparse_categorical_accuracy: 0.9684\n",
      "Epoch 23/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1015 - sparse_categorical_accuracy: 0.9689\n",
      "Epoch 24/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1022 - sparse_categorical_accuracy: 0.9689\n",
      "Epoch 25/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1015 - sparse_categorical_accuracy: 0.9693\n",
      "Epoch 26/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0992 - sparse_categorical_accuracy: 0.9707\n",
      "Epoch 27/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0972 - sparse_categorical_accuracy: 0.9705\n",
      "Epoch 28/50\n",
      "1313/1313 [==============================] - 10s 8ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9716\n",
      "Epoch 29/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9718\n",
      "Epoch 30/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0957 - sparse_categorical_accuracy: 0.9716\n",
      "Epoch 31/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0948 - sparse_categorical_accuracy: 0.9714\n",
      "Epoch 32/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9726\n",
      "Epoch 33/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0925 - sparse_categorical_accuracy: 0.9713\n",
      "Epoch 34/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0892 - sparse_categorical_accuracy: 0.9720\n",
      "Epoch 35/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0933 - sparse_categorical_accuracy: 0.9728\n",
      "Epoch 36/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0940 - sparse_categorical_accuracy: 0.9715\n",
      "Epoch 37/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0893 - sparse_categorical_accuracy: 0.9728\n",
      "Epoch 38/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0896 - sparse_categorical_accuracy: 0.9726\n",
      "Epoch 39/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0899 - sparse_categorical_accuracy: 0.9725\n",
      "Epoch 40/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0888 - sparse_categorical_accuracy: 0.9736\n",
      "Epoch 41/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0879 - sparse_categorical_accuracy: 0.9732\n",
      "Epoch 42/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0881 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 43/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0857 - sparse_categorical_accuracy: 0.9734\n",
      "Epoch 44/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0852 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 45/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0885 - sparse_categorical_accuracy: 0.9741\n",
      "Epoch 46/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0848 - sparse_categorical_accuracy: 0.9742\n",
      "Epoch 47/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0858 - sparse_categorical_accuracy: 0.9744\n",
      "Epoch 48/50\n",
      "1313/1313 [==============================] - 10s 8ms/step - loss: 0.0876 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 49/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0864 - sparse_categorical_accuracy: 0.9742\n",
      "Epoch 50/50\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0855 - sparse_categorical_accuracy: 0.9740\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = parse_train_data(TRAINING_FILE)\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    x=train_images,\n",
    "    y=train_labels,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "stop_at_accuracy_callback = StopAtAccuracy()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[stop_at_accuracy_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ca95f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.2928 - sparse_categorical_accuracy: 0.9176\n",
      "Epoch 2/5\n",
      "1313/1313 [==============================] - 2s 2ms/step - loss: 0.1328 - sparse_categorical_accuracy: 0.9601\n",
      "Epoch 3/5\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0915 - sparse_categorical_accuracy: 0.9731\n",
      "Epoch 4/5\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0684 - sparse_categorical_accuracy: 0.9794\n",
      "Epoch 5/5\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0526 - sparse_categorical_accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "# Save your model\n",
    "model = create_model()\n",
    "\n",
    "# Train your model\n",
    "history = model.fit(train_generator,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05829a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model as mnist.h5\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# Save model\n",
    "model.save('mnist.h5')\n",
    "print(\"Saving the model as mnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79805932",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model =tf.keras.models.load_model('mnist.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55ee8e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(train_images)\n",
    "y_pred = [np.argmax(i) for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad453327",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y2 =train_labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a7f73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_labels=np.argmax(train_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdec6652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d613b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(r_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61081313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cb0f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 19s 673us/step\n"
     ]
    }
   ],
   "source": [
    "test_images = parse_test_data(TEST_FILE)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow(\n",
    "    x=test_images,\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ImageId\": list(range(1, len(predicted_labels)+1)),\n",
    "    \"Label\": predicted_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ecd5a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images = parse_train_data(TEST_FILE)\n",
    "plt.imshow(test_images[6], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86354c21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20680\\414091045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Purples'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'values'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'matrix' is not defined"
     ]
    }
   ],
   "source": [
    "sns.heatmap(matrix, annot=True, cmap='Purples',fmt='g')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f828b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generators(test_images):\n",
    " \n",
    "  test_images = np.expand_dims(test_images, axis=3) #adding another dimension\n",
    "  \n",
    "\n",
    "  # Instantiate the ImageDataGenerator with image augmentation\n",
    "  test_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255 #rescale\n",
    "  )\n",
    "\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow method\n",
    "  test_generator = test_datagen.flow(x=test_images,\n",
    "                                       batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "  ### END CODE HERE\n",
    "\n",
    "  return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78294725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images of training generator have shape: (28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_generators(test_images)\n",
    "\n",
    "print(f\"Images of training generator have shape: {test_generator.x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0d7dc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 1s 720us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save the results as a CSV file\n",
    "import csv\n",
    "with open('submission.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ImageId', 'Label'])\n",
    "    for i, label in enumerate(predicted_labels):\n",
    "        writer.writerow([i + 1, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a20ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
